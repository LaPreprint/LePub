---
title: "Mathematical Theorems"
authors: 
  - name: Ralph Howard
    orcid: 0000-0003-2550-0012
    email: "howard@math.sc.edu"
    url: "www.math.sc.edu/~howard"
    affiliations: 
      - name: University of South Carolina
        department: Department of Mathematics
        city: Columbia
        state: SC
        postal-code: 29208
abstract: |
  How is speech like birdsong? What do we mean when we say an animal learns their vocalizations?
  Questions like these are answered by studying how animals communicate with sound. As in many other
  fields, the study of acoustic communication is being revolutionized by deep neural network models. These
  models enable answering questions that were previously impossible to address, in part because the models
  automate analysis of very large datasets. Acoustic communication researchers have developed multiple
  models for similar tasks, often implemented as research code with one of several libraries, such as Keras
  and Pytorch. This situation has created a real need for a framework that allows researchers to easily
  benchmark multiple models, and test new models, with their own data. To address this need, we developed
  vak (https://github.com/vocalpy/vak), a neural network framework designed for acoustic communication
  researchers. (“vak” is pronounced like “talk” or “squawk” and was chosen for its similarity to the Latin
  root voc, as in “vocal”.) Here we describe the design of the vak, and explain how the framework makes
  it easy for researchers to apply neural network models to their own data. We highlight enhancements
  made in version 1.0 that significantly improve user experience with the library. To provide researchers
  without expertise in deep learning access to these models, vak can be run via a command-line interface
  that uses configuration files. Vak can also be used directly in scripts by scientist-coders. To achieve this,
  vak adapts design patterns and an API from other domain-specific PyTorch libraries such as torchvision,
  with modules representing neural network operations, models, datasets, and transformations for pre- and
  post-processing. vak also leverages the Lightning library as a backend, so that vak developers and users
  can focus on the domain. We provide proof-of-concept results showing how vak can be used to test new
  models and compare existing models from multiple model families. In closing we discuss our roadmap for
  development and vision for the community of users.
acknowledgments: |
  I would like to thank a lot of people.
license:
  id: CC-BY-4.0
  name: CC-BY-4.0
  url: https://creativecommons.org/licenses/by/4.0/
github: https://github.com
open_access: true
keywords:
  - Mathemetics
  - Models
  - Equations
format:
  lepub-typst:
    keep-typ: true
    line-numbers: true
    theme-color: "#FF5733"
    margin-side: right
    part-funding: |
      This work was supported by the Swiss National Science Foundation (SNSF; https://www.snf.ch/en) Ambizione Fellowship PZ00P3_180085 (to AH), SNSF Starting Grant TMSGI3_211369 (to AH), and SNSF National Centre of Competence in Research (NCCR) AntiResist (to AH; grant number 180541). The funder played no role in the study design, data collection and analysis, decision to publish, or preparation of the manuscript.
    part-data-availability: |
      Data is available a cool place.
bibliography: refs.bib
bibliographystyle: vancouver-brackets.csl
---

## Introduction

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur et ornare urna, ornare malesuada ante. Aenean fringilla fermentum pulvinar. Integer maximus mauris id gravida eleifend. Donec et facilisis neque [@netwok2020].

Vestibulum massa nunc, pellentesque vitae velit vitae, porttitor efficitur nisl. Integer massa lacus, tristique a laoreet vitae, commodo sed odio. Phasellus eget metus tempus sem molestie iaculis. 

## Mathematics

This formula is also referred to as the _binomial formula_ or the _binomial identity_. It can be written as:

$$
(x+y)^n = \sum_{k=0}^n {n \choose k}x^{n-k}y^k = \sum_{k=0}^n {n \choose k}x^{k}y^{n-k}
$$

::: {.ams-theorem numbered="true"}
The square of any real number is non-negative.
:::

::: {.ams-proof}
Any real number $x$ satisfies $x > 0$, $x = 0$, or $x < 0$. If $x = 0$,
then $x^2 = 0 >= 0$. If $x > 0$ then as a positive time a positive is
positive we have $x^2 = x x > 0$. If $x < 0$ then $−x > 0$ and so by
what we have just done $x^2 = (−x)^2 > 0$. So in all cases $x^2 ≥ 0$.
:::

## Summary

Aenean sagittis, nisl et egestas facilisis, mauris erat pulvinar turpis, tincidunt maximus tellus erat in ligula. Phasellus vitae lacinia nisi. Aliquam aliquam feugiat lectus sed congue. Ut ut molestie lacus. Nam id felis aliquam, pulvinar erat eu, luctus ligula. Vestibulum quis metus accumsan.